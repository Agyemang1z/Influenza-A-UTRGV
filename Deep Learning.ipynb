{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94dd80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymannkendall\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ce52f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\edmun\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU,SimpleRNN, Dense, Bidirectional,Dropout, TimeDistributed\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop,SGD\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97bf8c3-2266-474b-b1a2-3cbc4cea8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your time series data\n",
    "time_series_data = pd.read_csv(r\"C:\\Users\\edmun\\Desktop\\UTRGV Thesis\\ARIMA_ETS\\Data and Codes\\USA.csv\")\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "time_series_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e99d9",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e08bb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for RNN\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_data['FluAcases'].values.reshape(-1, 1))\n",
    "\n",
    "train_size = int(len(scaled_data) * 0.933)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "\n",
    "### You can also use this for both train and test data\n",
    "# Train: Jan 2009 - Dec 2022\n",
    "train_data = scaled_data['2009-01-01':'2022-12-01']\n",
    "\n",
    "# Test: Jan 2023 - Dec 2023\n",
    "test_data = scaled_data['2023-01-01':'2023-12-01']\n",
    "\n",
    "print(f\"Train shape: {train_data.shape}, Test shape: {test_data.shape}\")\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 12\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9762943",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467fb0a-5332-4e21-a19b-4849da25f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [16, 32, 64, 128, 256, 512]  # Number of units in LSTM\n",
    "activation_functions = [\"relu\", \"tanh\", \"sigmoid\"]  # Activation functions\n",
    "learning_rates = [0.001, 0.01,0.1]  # Learning rates\n",
    "optimizers = [\"adam\", \"rmsprop\", \"sgd\"]  # Optimizers\n",
    "batch_sizes = [16, 32, 64,128]  # Batch sizes\n",
    "\n",
    "best_score = float(\"inf\")\n",
    "best_params = {}\n",
    "\n",
    "for unit in units:\n",
    "    for activation in activation_functions:\n",
    "        for lr in learning_rates:\n",
    "            for opt in optimizers:\n",
    "                for batch_size in batch_sizes:\n",
    "                    model = Sequential()\n",
    "                    model.add(LSTM(units=unit, activation=activation, input_shape=(time_step, 1), return_sequences=True))\n",
    "                    model.add(Dropout(0.3))\n",
    "                    model.add(LSTM(units=unit, return_sequences=True))\n",
    "                    model.add(Dropout(0.3))\n",
    "                    model.add(TimeDistributed(Dense(1)))\n",
    "                    \n",
    "                    if opt == \"adam\":\n",
    "                        optimizer = Adam(learning_rate=lr)\n",
    "                    elif opt == \"rmsprop\":\n",
    "                        optimizer = RMSprop(learning_rate=lr)\n",
    "                    elif opt == \"sgd\":\n",
    "                        optimizer = SGD(learning_rate=lr)\n",
    "                    \n",
    "                    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "                    \n",
    "                    history = model.fit(X_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2, shuffle=False, verbose=1)\n",
    "                    \n",
    "                    val_mse = min(history.history['val_mean_squared_error'])\n",
    "                    \n",
    "                    if val_mse < best_score:\n",
    "                        best_score = val_mse\n",
    "                        best_params = {\n",
    "                            \"units\": unit,\n",
    "                            \"activation\": activation,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"batch_size\": batch_size\n",
    "                        }\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "print(\"Best validation MSE: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee555609",
   "metadata": {},
   "source": [
    "## Gated Recurrent Neural Network (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfceb2-c655-4a9e-a845-f88040fb86d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [16, 32, 64, 128, 256, 512]  # Number of units in GRU\n",
    "activation_functions = [\"relu\", \"tanh\", \"sigmoid\"]  # Activation functions\n",
    "learning_rates = [0.001, 0.01,0.1]  # Learning rates\n",
    "optimizers = [\"adam\", \"rmsprop\", \"sgd\"]  # Optimizers\n",
    "batch_sizes = [16, 32, 64,128]  # Batch sizes\n",
    "\n",
    "best_score = float(\"inf\")\n",
    "best_params = {}\n",
    "\n",
    "for unit in units:\n",
    "    for activation in activation_functions:\n",
    "        for lr in learning_rates:\n",
    "            for opt in optimizers:\n",
    "                for batch_size in batch_sizes:\n",
    "                    model = Sequential()\n",
    "                    model.add(GRU(units=unit, activation=activation, input_shape=(time_step, 1), return_sequences=True))\n",
    "                    model.add(Dropout(0.5))\n",
    "                    model.add(GRU(units=unit, return_sequences=True))\n",
    "                    model.add(Dropout(0.5))\n",
    "                    model.add(TimeDistributed(Dense(1)))\n",
    "                    \n",
    "                    if opt == \"adam\":\n",
    "                        optimizer = Adam(learning_rate=lr)\n",
    "                    elif opt == \"rmsprop\":\n",
    "                        optimizer = RMSprop(learning_rate=lr)\n",
    "                    elif opt == \"sgd\":\n",
    "                        optimizer = SGD(learning_rate=lr)\n",
    "                    \n",
    "                    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "                    \n",
    "                    history = model.fit(X_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2, shuffle=False, verbose=1)\n",
    "                    \n",
    "                    val_mse = min(history.history['val_mean_squared_error'])\n",
    "                    \n",
    "                    if val_mse < best_score:\n",
    "                        best_score = val_mse\n",
    "                        best_params = {\n",
    "                            \"units\": unit,\n",
    "                            \"activation\": activation,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"batch_size\": batch_size\n",
    "                        }\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best validation MSE: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848aac0a",
   "metadata": {},
   "source": [
    "## Simple Recurrent Neural Network (SimpleRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [16, 32, 64, 128, 256, 512] # Number of units in SimpleRnn\n",
    "activation_functions = [\"relu\", \"tanh\", \"sigmoid\"]  # Activation functions\n",
    "learning_rates = [0.001, 0.01,0.1]  # Learning rates\n",
    "optimizers = [\"adam\", \"rmsprop\", \"sgd\"]  # Optimizers\n",
    "batch_sizes = [16, 32, 64,128]  # Batch sizes\n",
    "\n",
    "best_score = float(\"inf\")\n",
    "best_params = {}\n",
    "\n",
    "for unit in units:\n",
    "    for activation in activation_functions:\n",
    "        for lr in learning_rates:\n",
    "            for opt in optimizers:\n",
    "                for batch_size in batch_sizes:\n",
    "                    model = Sequential()\n",
    "                    model.add(SimpleRNN(units=unit, activation=activation, input_shape=(time_step, 1), return_sequences=True))\n",
    "                    model.add(Dropout(0.3))\n",
    "                    model.add(SimpleRNN(units=unit, return_sequences=True))\n",
    "                    model.add(Dropout(0.3))\n",
    "                    model.add(TimeDistributed(Dense(1)))\n",
    "                    \n",
    "                    if opt == \"adam\":\n",
    "                        optimizer = Adam(learning_rate=lr)\n",
    "                    elif opt == \"rmsprop\":\n",
    "                        optimizer = RMSprop(learning_rate=lr)\n",
    "                    elif opt == \"sgd\":\n",
    "                        optimizer = SGD(learning_rate=lr)\n",
    "                    \n",
    "                    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "                    \n",
    "                    history = model.fit(X_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2, shuffle=False, verbose=1)\n",
    "                    \n",
    "                    val_mse = min(history.history['val_mean_squared_error'])\n",
    "                    \n",
    "                    if val_mse < best_score:\n",
    "                        best_score = val_mse\n",
    "                        best_params = {\n",
    "                            \"units\": unit,\n",
    "                            \"activation\": activation,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"batch_size\": batch_size\n",
    "                        }\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best validation MSE: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cbfc65-6ad8-430e-9fde-c4176487770a",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098549c4-45fc-4e02-b033-50d95c145481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dropout, TimeDistributed, Dense\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "units = [16, 32, 64, 128, 256, 512]  # Number of units in LSTM\n",
    "activation_functions = [\"relu\", \"tanh\", \"sigmoid\"]  # Activation functions (though 'tanh' is standard for LSTM)\n",
    "learning_rates = [0.001, 0.01, 0.1]  # Learning rates\n",
    "optimizers = [\"adam\", \"rmsprop\", \"sgd\"]  # Optimizers\n",
    "batch_sizes = [16, 32, 64, 128]  # Batch sizes\n",
    "\n",
    "best_score = float(\"inf\")\n",
    "best_params = {}\n",
    "\n",
    "for unit in units:\n",
    "    for activation in activation_functions:\n",
    "        for lr in learning_rates:\n",
    "            for opt in optimizers:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Construct the model\n",
    "                    model = Sequential()\n",
    "                    model.add(Bidirectional(LSTM(units=unit, activation=activation, return_sequences=True), input_shape=(time_step, 1)))\n",
    "                    model.add(Dropout(0.3))\n",
    "                    model.add(Bidirectional(LSTM(units=unit, activation=activation, return_sequences=True)))\n",
    "                    model.add(Dropout(0.3))\n",
    "                    model.add(TimeDistributed(Dense(1)))\n",
    "                    \n",
    "                    # Pick optimizer based on loop\n",
    "                    if opt == \"adam\":\n",
    "                        optimizer = Adam(learning_rate=lr)\n",
    "                    elif opt == \"rmsprop\":\n",
    "                        optimizer = RMSprop(learning_rate=lr)\n",
    "                    elif opt == \"sgd\":\n",
    "                        optimizer = SGD(learning_rate=lr)\n",
    "                    \n",
    "                    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "                    \n",
    "                    # Train the model briefly\n",
    "                    history = model.fit(\n",
    "                        X_train, y_train,\n",
    "                        epochs=5,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        shuffle=False,\n",
    "                        verbose=1\n",
    "                    )\n",
    "                    \n",
    "                    # Grab the best validation score\n",
    "                    val_mse = min(history.history['val_mean_squared_error'])\n",
    "                    \n",
    "                    # Save best params if this is the new best\n",
    "                    if val_mse < best_score:\n",
    "                        best_score = val_mse\n",
    "                        best_params = {\n",
    "                            \"units\": unit,\n",
    "                            \"activation\": activation,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"batch_size\": batch_size\n",
    "                        }\n",
    "\n",
    "print(\"Best parameters found:\", best_params)\n",
    "print(\"Best validation MSE:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c684a7-e7ee-49d9-b735-56124cb25e65",
   "metadata": {},
   "source": [
    "### BiGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa28f9-e852-43b8-a5f7-07722be13f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, GRU, Dropout, TimeDistributed, Dense\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "units = [16, 32, 64, 128, 256, 512]  # Number of units in GRU\n",
    "activation_functions = [\"relu\", \"tanh\", \"sigmoid\"]  # GRUs usually use 'tanh', but you can play with these\n",
    "learning_rates = [0.001, 0.01, 0.1]  # Learning rates\n",
    "optimizers = [\"adam\", \"rmsprop\", \"sgd\"]  # Optimizers\n",
    "batch_sizes = [16, 32, 64, 128]  # Batch sizes\n",
    "\n",
    "best_score = float(\"inf\")\n",
    "best_params = {}\n",
    "\n",
    "for unit in units:\n",
    "    for activation in activation_functions:\n",
    "        for lr in learning_rates:\n",
    "            for opt in optimizers:\n",
    "                for batch_size in batch_sizes:\n",
    "                    model = Sequential()\n",
    "                    model.add(\n",
    "                        Bidirectional(\n",
    "                            GRU(units=unit, activation=activation, return_sequences=True),\n",
    "                            input_shape=(time_step, 1)\n",
    "                        )\n",
    "                    )\n",
    "                    model.add(Dropout(0.3))\n",
    "                    model.add(\n",
    "                        Bidirectional(\n",
    "                            GRU(units=unit, activation=activation, return_sequences=True)\n",
    "                        )\n",
    "                    )\n",
    "                    model.add(Dropout(0.3))\n",
    "                    model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "                    # Set up optimizer\n",
    "                    if opt == \"adam\":\n",
    "                        optimizer = Adam(learning_rate=lr)\n",
    "                    elif opt == \"rmsprop\":\n",
    "                        optimizer = RMSprop(learning_rate=lr)\n",
    "                    elif opt == \"sgd\":\n",
    "                        optimizer = SGD(learning_rate=lr)\n",
    "\n",
    "                    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "\n",
    "                    # Fit model, just 5 epochs for quick grid search\n",
    "                    history = model.fit(\n",
    "                        X_train, y_train,\n",
    "                        epochs=5,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        shuffle=False,\n",
    "                        verbose=1\n",
    "                    )\n",
    "\n",
    "                    val_mse = min(history.history['val_mean_squared_error'])\n",
    "\n",
    "                    # Save best result\n",
    "                    if val_mse < best_score:\n",
    "                        best_score = val_mse\n",
    "                        best_params = {\n",
    "                            \"units\": unit,\n",
    "                            \"activation\": activation,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"batch_size\": batch_size\n",
    "                        }\n",
    "\n",
    "print(\"Best parameters found:\", best_params)\n",
    "print(\"Best validation MSE:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2869e-aed6-4421-a7a0-c324f8996043",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2db324-1c8c-4856-a3f6-f6b20169e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a4495-7ebc-4cd0-b3a8-c23d3c69938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Scaled Dot-Product Attention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn, value)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e3fb7-fca7-4ddf-af8d-6ae8050ab761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        query = self.query_linear(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        key = self.key_linear(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        value = self.value_linear(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_output, attn = self.attention(query, key, value, mask)\n",
    "\n",
    "        # Concatenate and apply final linear layer\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.out_linear(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb76cd0-6cf8-4db0-b614-d8c6b4077ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa290a64-f2c9-4428-bec1-595fb1995bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.3):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(num_heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        # Self-attention and Add & Norm\n",
    "        src2 = self.self_attn(src, src, src, mask)\n",
    "        src = self.layer_norm1(src + self.dropout(src2))\n",
    "\n",
    "        # Feed Forward and Add & Norm\n",
    "        src2 = self.feed_forward(src)\n",
    "        src = self.layer_norm2(src + self.dropout(src2))\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab2b77-5455-4a1d-b850-25149dad1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, max_seq_len, dropout=0.3):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        seq_len = src.size(1)\n",
    "        src = self.embedding(src) + self.pos_embedding[:, :seq_len, :]\n",
    "        src = self.dropout(src)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, mask)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056181e-2020-4d03-9938-081133293d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Function to load and preprocess the data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Load the data from CSV file\n",
    "    Influenza_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert 'Start Date' to datetime and set as index\n",
    "    Influenza_data['Date'] = pd.to_datetime(Influenza_data['Date'])\n",
    "    Influenza_data.set_index('Date', inplace=True)\n",
    "\n",
    "    # Select the 'Percent Expected COVID-19 Deaths' and fill missing values\n",
    "    death_percentage = Influenza_data['FluAcases'].fillna(0)\n",
    "\n",
    "    # Scale the data between 0 and 1\n",
    "    scaler = MinMaxScaler()\n",
    "    death_percentage_scaled = scaler.fit_transform(death_percentage.values.reshape(-1, 1))\n",
    "\n",
    "    return death_percentage_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110d590-9ff4-4e60-a277-9efed9eeb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for time-series forecasting\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append((data[i:i+seq_length], data[i+seq_length]))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e329b5f-492b-4277-bd4a-7716f80e3467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sequences to PyTorch tensors\n",
    "def to_tensor(data):\n",
    "    # Convert list of sequences into a single numpy array\n",
    "    X_np = np.array([seq[0] for seq in data])\n",
    "    y_np = np.array([seq[1] for seq in data])\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X = torch.tensor(X_np, dtype=torch.float32)\n",
    "    y = torch.tensor(y_np, dtype=torch.float32)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae6ad1-ef5c-4c89-b8a6-d9b7f7d77c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to load data, preprocess, and create sequences\n",
    "def prepare_data_for_transformer(file_path, seq_length=12):\n",
    "    # Load and preprocess the data\n",
    "    data_scaled, scaler = load_and_preprocess_data(file_path)\n",
    "\n",
    "    # Create sequences\n",
    "    sequences = create_sequences(data_scaled, seq_length)\n",
    "\n",
    "    # Split into training and test sets (80% train, 20% test)\n",
    "    train_size = int(len(sequences) * 0.933)\n",
    "    train_sequences = sequences[:train_size]\n",
    "    test_sequences = sequences[train_size:]\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train, y_train = to_tensor(train_sequences)\n",
    "    X_test, y_test = to_tensor(test_sequences)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf3981-0b22-458d-a3c7-94394e09f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "file_path = r\"C:\\Users\\edmun\\Desktop\\UTRGV Thesis\\TS Manuscript Prof - Copy\\Data and Codes\\USA.csv\" # Update with your file path\n",
    "X_train, y_train, X_test, y_test, scaler = prepare_data_for_transformer(file_path)\n",
    "\n",
    "# Check the shapes of the tensors\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25bd5d-25ac-4a9b-8493-f15b2a6bc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9ee12-5b8f-4c13-a29d-e8a981fb76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted Transformer Encoder for time-series forecasting\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, seq_len, dropout=0.3):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "        self.input_linear = nn.Linear(1, d_model)  # From 1D input to d_model dimension\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, d_model))  # Positional encoding\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(d_model, 1)  # Final output to predict next time step value\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        # src: (batch_size, seq_len, input_dim)\n",
    "        src = self.input_linear(src)  # Linear transformation\n",
    "        seq_len = src.size(1)\n",
    "        src = src + self.pos_embedding[:, :seq_len, :]  # Add positional encoding\n",
    "        src = self.dropout(src)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, mask)\n",
    "\n",
    "        # Predict the next time step value\n",
    "        output = self.fc_out(src[:, -1, :])  # Only take the last time step for prediction\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9fdf7a-0c4e-4b0e-b1fc-ef020be1f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the transformer model\n",
    "def train_transformer(model, train_loader, num_epochs=50, lr=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(X_batch)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Preparing COVID-19 data for training\n",
    "def prepare_dataloader(X_train, y_train, batch_size=32):\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming X_train, y_train are already loaded and preprocessed (from the steps you provided earlier)\n",
    "    seq_len = X_train.shape[1]  # e.g., 4 (the sequence length)\n",
    "    d_model = 64  # Embedding dimension\n",
    "    num_heads = 3\n",
    "    num_layers = 4\n",
    "    d_ff = 128  # Feed-forward hidden layer size\n",
    "    dropout = 0.3\n",
    "    num_epochs = 50\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TransformerTimeSeries(num_layers, d_model, num_heads, d_ff, seq_len, dropout)\n",
    "\n",
    "    # Prepare the dataloader\n",
    "    train_loader = prepare_dataloader(X_train, y_train, batch_size=32)\n",
    "\n",
    "    # Train the model\n",
    "    train_transformer(model, train_loader, num_epochs=num_epochs, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29ee81-5113-4795-ab45-a1afdc007f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to make predictions on the training and testing sets\n",
    "def predict(model, X_data):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to calculate gradients during inference\n",
    "        predictions = model(X_data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b7cdf-6946-45bf-96a3-4e5563db24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model performance on training and testing sets\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Predictions\n",
    "    train_predictions = predict(model, X_train)\n",
    "    test_predictions = predict(model, X_test)\n",
    "\n",
    "    # Calculate loss\n",
    "    train_loss = criterion(train_predictions, y_train).item()\n",
    "    test_loss = criterion(test_predictions, y_test).item()\n",
    "\n",
    "    print(f'Training Loss (MSE): {train_loss:.6f}')\n",
    "    print(f'Test Loss (MSE): {test_loss:.6f}')\n",
    "\n",
    "    return train_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f633b-717a-4de7-ab9b-b27ab8357cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(train_predictions, y_train, test_predictions, y_test, scaler):\n",
    "    # Generate date range for the data points\n",
    "    start_date = pd.Timestamp('2023-01-01')\n",
    "    end_date = pd.Timestamp('2023-12-01')\n",
    "    dates = pd.date_range(start=start_date, end=end_date, periods=len(y_train) + len(y_test))\n",
    "\n",
    "    # Convert predictions and true values back to the original scale\n",
    "    train_predictions_rescaled = scaler.inverse_transform(train_predictions.detach().numpy())\n",
    "    y_train_rescaled = scaler.inverse_transform(y_train.detach().numpy())\n",
    "    test_predictions_rescaled = scaler.inverse_transform(test_predictions.detach().numpy())\n",
    "    y_test_rescaled = scaler.inverse_transform(y_test.detach().numpy())\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot training data\n",
    "    plt.plot(dates[:len(y_train_rescaled)], y_train_rescaled, label='Actual Training Data', color='blue')\n",
    "    plt.plot(dates[:len(train_predictions_rescaled)], train_predictions_rescaled, label='Predicted Training Data', color='orange', linestyle='--')\n",
    "\n",
    "    # Plot testing data\n",
    "    plt.plot(dates[len(y_train_rescaled):len(y_train_rescaled) + len(y_test_rescaled)], y_test_rescaled, label='Actual Test Data', color='green')\n",
    "    plt.plot(dates[len(y_train_rescaled):len(y_train_rescaled) + len(test_predictions_rescaled)], test_predictions_rescaled, label='Predicted Test Data', color='red', linestyle='--')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Influenza A Cases')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage of evaluation and plotting functions\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume you have already trained the model and you have X_train, y_train, X_test, y_test, and the scaler\n",
    "    train_predictions, test_predictions = evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Plot the predictions on top of actual data\n",
    "    plot_predictions(train_predictions, y_train, test_predictions, y_test, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a3104-d89e-4f9c-84a4-96f70b4a6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_metrics(predictions, actuals):\n",
    "    # Ensure the predictions and actuals are float tensors\n",
    "    predictions = predictions.float()\n",
    "    actuals = actuals.float()\n",
    "\n",
    "    # MAE\n",
    "    mae = torch.mean(torch.abs(actuals - predictions))\n",
    "    \n",
    "    # MSE\n",
    "    mse = torch.mean((actuals - predictions) ** 2)\n",
    "\n",
    "    # GMRAE\n",
    "    gmrae = torch.exp(torch.mean(torch.abs(torch.log1p(actuals) - torch.log1p(predictions))))\n",
    "\n",
    "    # Theil's U1 statistic\n",
    "    errors = actuals - predictions\n",
    "    errors_squared = errors ** 2\n",
    "    numerator = torch.sqrt(torch.mean(errors_squared))\n",
    "    denominator = torch.sqrt(torch.mean(actuals ** 2)) + torch.sqrt(torch.mean(predictions ** 2))\n",
    "    theil_u1 = numerator / denominator\n",
    "    \n",
    "    # RMSE\n",
    "    rmse = torch.sqrt(torch.mean((actuals - predictions) ** 2))\n",
    "    \n",
    "    # MAPE\n",
    "    mape = torch.mean(torch.abs((actuals - predictions) / (actuals + 1e-8))) * 100  # Added epsilon to avoid division by zero\n",
    "    \n",
    "    # SMAPE\n",
    "    smape = 100 * torch.mean(2 * torch.abs(actuals - predictions) / (torch.abs(actuals) + torch.abs(predictions) + 1e-8))\n",
    "\n",
    "    return mae, mse, gmrae, theil_u1, rmse, mape, smape\n",
    "    \n",
    "\n",
    "# Assuming train_predictions, y_train, test_predictions, y_test are PyTorch tensors\n",
    "train_metrics = calculate_metrics(train_predictions, y_train)\n",
    "test_metrics = calculate_metrics(test_predictions, y_test)\n",
    "\n",
    "print(f\"Training Metrics: MAE: {train_metrics[0].item():.4f}, MSE: {train_metrics[1].item():.4f}, GMRAE: {train_metrics[2].item():.4f}, Theil's U1: {train_metrics[3].item():.4f}, RMSE: {train_metrics[4].item():.4f}, MAPE: {train_metrics[5].item():.2f}%, SMAPE: {train_metrics[6].item():.2f}%\")\n",
    "print(f\"Testing Metrics: MAE: {test_metrics[0].item():.4f}, MSE: {test_metrics[1].item():.4f}, GMRAE: {test_metrics[2].item():.4f}, Theil's U1: {test_metrics[3].item():.4f}, RMSE: {test_metrics[4].item():.4f}, MAPE: {test_metrics[5].item():.2f}%, SMAPE: {test_metrics[6].item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653f6ab-8af7-49ab-9216-7ea47f2cbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "file_path = r\"C:\\Users\\edmun\\Desktop\\UTRGV Thesis\\TS Manuscript Prof - Copy\\Data and Codes\\USA.csv\"  # Update with your file path\n",
    "X_train, y_train, X_test, y_test, scaler = prepare_data_for_transformer(file_path)\n",
    "\n",
    "# Check the shapes of the tensors\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Function to create the LSTM model\n",
    "class LSTMTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim=1):\n",
    "        super(LSTMTimeSeries, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "# Adjust this function to train the model\n",
    "def train_lstm(model, train_loader, num_epochs=50, lr=0.01):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "# Example of creating and training the model\n",
    "if __name__ == \"__main__\":\n",
    "    input_dim = 1  # number of features\n",
    "    hidden_dim = 64  # number of features in hidden state\n",
    "    num_layers = 4  # number of stacked lstm layers\n",
    "    output_dim = 1  # number of output classes\n",
    "    d_ff = 128  # Feed-forward hidden layer size\n",
    "    dropout = 0.3\n",
    "    num_epochs = 50\n",
    "\n",
    "    model = LSTMTimeSeries(input_dim, hidden_dim, num_layers, output_dim)\n",
    "    train_loader = prepare_dataloader(X_train, y_train, batch_size=32)\n",
    "    train_lstm(model, train_loader)\n",
    "### Repeat for other RNN models by changing the network and its associated hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
